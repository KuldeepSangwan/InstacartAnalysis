{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying predicting None with other Products and F1 optimization approche to improve f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Idea is we gonne create two different models one that we have already created that predicts products and the other one is to predict None(we are gonne consider none as product and gonne predict it with other products)</p>\n",
    "<p>Step 1 - First get the features for None model, I have selected subset of the features from the previous Model we had</p>\n",
    "<p>Step 2 - Apply Gradient Boosting Model after choosing best parameters, then predict the Test data and save the probablities into a file</p>\n",
    "<p>Step 3 - get the probablities for the Gradient Boosting model that we applied before.</p>\n",
    "<p>Step 4 - So now we would be having probablities from two different model, now we need to merge these probablities and apply F1 optimization to this to choose best f1 score from given probablity values, that will give the top prodicts for a order with None as product included.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Data\"\n",
    "order_train = pd.read_csv(os.path.join(path, \"order_products__train.csv\"), dtype={'order_id': np.uint32,\n",
    "                                                                                  'product_id': np.uint16,\n",
    "                                                                                  'add_to_cart_order':np.uint8,\n",
    "                                                                                  'reordered': bool})\n",
    "orders = pd.read_csv(os.path.join(path, \"orders.csv\"), dtype={'order_id':np.uint32,\n",
    "                                                              'user_id': np.uint32,\n",
    "                                                              'eval_set': 'category',\n",
    "                                                              'order_number':np.uint8,\n",
    "                                                              'order_dow': np.uint8,\n",
    "                                                              'order_hour_of_day': np.uint8\n",
    "                                                              })\n",
    "order_prior = pd.read_csv(os.path.join(path, \"order_products__prior.csv\"), dtype={'order_id': np.uint32,\n",
    "                                                                                  'product_id': np.uint16,\n",
    "                                                                                  'add_to_cart_order':np.uint8,\n",
    "                                                                                  'reordered': bool})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### why do we need to predict None ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of orders that are without any reorder in Train:  6.555952716658156 %\n"
     ]
    }
   ],
   "source": [
    "order = orders[orders['eval_set']==\"train\"]\n",
    "merged = order.merge(order_train, on='order_id')\n",
    "here = merged.groupby(by='order_id').agg({'reordered':'sum'})\n",
    "number_none_orders = here[here['reordered']==0].shape[0]\n",
    "print('Percentage of orders that are without any reorder in Train: ',(number_none_orders/order.shape[0])*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Train and Test Data for None Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('my_final/data.csv')[['user_id','order_id','NumberTimeProductsOrdered','unique_prod','user_orders_count','order_dow','order_hour_of_day','days_since_prior_order','user_reorder_ratio','order_dow_mean','order_hour_of_day_mean','days_since_prior_order_mean','Avergae_Basket_size']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data\n",
    "data_train1 = data_train.copy()\n",
    "data_train1=data_train1.drop_duplicates()\n",
    "data_train1 = data_train1.groupby('user_id').agg('mean')\n",
    "data_train1['user_id']=data_train1.index\n",
    "#reading labels\n",
    "order = orders[orders['eval_set']==\"train\"]\n",
    "merged = order.merge(order_train, on='order_id')\n",
    "here = merged.groupby(by='order_id').agg({'reordered':'sum'})\n",
    "here.loc[here['reordered']>0,'reordered']=1\n",
    "here = here.rename(columns={'reordered':'is_none'})\n",
    "ok = here['is_none'].values\n",
    "okk = np.where(ok>0,0,1)\n",
    "here['is_none']=okk\n",
    "data_train1 = data_train1.merge(here, on='order_id')\n",
    "labels_train = data_train1['is_none']\n",
    "\n",
    "# filling NaN values as -1\n",
    "data_train1 = data_train1.fillna(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding one more feature to train: none_order_rate\n",
    "\n",
    "merged = orders.merge(order_prior, on='order_id')\n",
    "merged = merged[merged['order_number']!=1]\n",
    "here = merged.groupby(by='order_id').agg({'reordered':'sum'})\n",
    "ok = merged[['order_id','user_id','eval_set']].merge(here,on='order_id', how='inner').drop_duplicates()\n",
    "ok.loc[ok['reordered']>0,'reordered']=1\n",
    "ok1 = ok.groupby('user_id').agg({'reordered':['sum','count']})\n",
    "ok1.columns = ok1.columns.levels[1]\n",
    "ok1['none_order_rate']= (ok1['count']-ok1['sum'])/ok1['count']\n",
    "\n",
    "data_train1 = data_train1.merge(ok1[['none_order_rate']],on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test data\n",
    "data_test1 = pd.read_csv('my_final/test_data.csv')[['user_id','order_id','NumberTimeProductsOrdered','unique_prod','user_orders_count','order_dow','order_hour_of_day','days_since_prior_order','user_reorder_ratio','order_dow_mean','order_hour_of_day_mean','days_since_prior_order_mean','Avergae_Basket_size']]\n",
    "data_test1=data_test1.drop_duplicates()\n",
    "data_test1 = data_test1.groupby('user_id').agg('mean')\n",
    "\n",
    "# filling NaN values as -1\n",
    "data_test1 = data_test1.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding one more feature to test: none_order_rate\n",
    "\n",
    "merged = orders.merge(order_prior, on='order_id')\n",
    "merged = merged[merged['order_number']!=1]\n",
    "here = merged.groupby(by='order_id').agg({'reordered':'sum'})\n",
    "ok = merged[['order_id','user_id','eval_set']].merge(here,on='order_id', how='inner').drop_duplicates()\n",
    "ok.loc[ok['reordered']>0,'reordered']=1\n",
    "ok1 = ok.groupby('user_id').agg({'reordered':['sum','count']})\n",
    "ok1.columns = ok1.columns.levels[1]\n",
    "ok1['none_order_rate']= (ok1['count']-ok1['sum'])/ok1['count']\n",
    "\n",
    "data_test1 = data_test1.merge(ok1[['none_order_rate']],on='user_id')\n",
    "data_test1['user_id']= data_test1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save data files\n",
    "# data_train1.to_csv('my_final/train_none_Data.csv')\n",
    "# data_test1.to_csv('my_final/test_none_Data.csv')\n",
    "\n",
    "# #read Data files\n",
    "# data_train1 = pd.read_csv('my_final/train_none_Data.csv')\n",
    "# data_test1 = pd.read_csv('my_final/test_none_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting model for None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyper-parameter tuning for None model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005 0.1 4 0.7 0.8 1 0.5 reg:logistic\n",
      "logloss 0.211985 for 499 rounds\n",
      "0.005 0.1 4 0.7 0.8 1 0.5 reg:squarederror\n",
      "logloss 0.21062266666666665 for 499 rounds\n",
      "0.005 0.1 4 0.7 0.8 1 1 reg:logistic\n",
      "logloss 0.21202600000000002 for 499 rounds\n",
      "0.005 0.1 4 0.7 0.8 1 1 reg:squarederror\n",
      "logloss 0.21065366666666666 for 499 rounds\n",
      "0.005 0.1 4 0.7 0.8 5 0.5 reg:logistic\n",
      "logloss 0.2122383333333333 for 499 rounds\n",
      "0.005 0.1 4 0.7 0.8 5 0.5 reg:squarederror\n",
      "logloss 0.21065466666666666 for 499 rounds\n",
      "0.005 0.1 4 0.7 0.8 5 1 reg:logistic\n",
      "logloss 0.21229333333333333 for 499 rounds\n",
      "0.005 0.1 4 0.7 0.8 5 1 reg:squarederror\n",
      "logloss 0.21067866666666668 for 499 rounds\n",
      "0.005 0.1 4 0.7 0.9 1 0.5 reg:logistic\n",
      "logloss 0.21199400000000002 for 499 rounds\n",
      "0.005 0.1 4 0.7 0.9 1 0.5 reg:squarederror\n",
      "logloss 0.210615 for 499 rounds\n",
      "0.005 0.1 4 0.7 0.9 1 1 reg:logistic\n",
      "logloss 0.21203466666666668 for 499 rounds\n",
      "0.005 0.1 4 0.7 0.9 1 1 reg:squarederror\n",
      "logloss 0.21064366666666667 for 499 rounds\n",
      "0.005 0.1 4 0.7 0.9 5 0.5 reg:logistic\n",
      "logloss 0.21225566666666665 for 499 rounds\n",
      "0.005 0.1 4 0.7 0.9 5 0.5 reg:squarederror\n",
      "logloss 0.21064333333333332 for 499 rounds\n",
      "0.005 0.1 4 0.7 0.9 5 1 reg:logistic\n",
      "logloss 0.21229733333333334 for 499 rounds\n",
      "0.005 0.1 4 0.7 0.9 5 1 reg:squarederror\n",
      "logloss 0.21067066666666667 for 499 rounds\n",
      "0.005 0.1 4 0.8 0.8 1 0.5 reg:logistic\n",
      "logloss 0.21202533333333332 for 499 rounds\n",
      "0.005 0.1 4 0.8 0.8 1 0.5 reg:squarederror\n",
      "logloss 0.21064000000000002 for 499 rounds\n",
      "0.005 0.1 4 0.8 0.8 1 1 reg:logistic\n",
      "logloss 0.21206233333333335 for 499 rounds\n",
      "0.005 0.1 4 0.8 0.8 1 1 reg:squarederror\n",
      "logloss 0.21066600000000002 for 499 rounds\n",
      "0.005 0.1 4 0.8 0.8 5 0.5 reg:logistic\n",
      "logloss 0.21225133333333335 for 499 rounds\n",
      "0.005 0.1 4 0.8 0.8 5 0.5 reg:squarederror\n",
      "logloss 0.210664 for 499 rounds\n",
      "0.005 0.1 4 0.8 0.8 5 1 reg:logistic\n",
      "logloss 0.21229633333333334 for 499 rounds\n",
      "0.005 0.1 4 0.8 0.8 5 1 reg:squarederror\n",
      "logloss 0.21068833333333334 for 499 rounds\n",
      "0.005 0.1 4 0.8 0.9 1 0.5 reg:logistic\n",
      "logloss 0.21204133333333333 for 499 rounds\n",
      "0.005 0.1 4 0.8 0.9 1 0.5 reg:squarederror\n",
      "logloss 0.2106353333333333 for 499 rounds\n",
      "0.005 0.1 4 0.8 0.9 1 1 reg:logistic\n",
      "logloss 0.21207433333333334 for 499 rounds\n",
      "0.005 0.1 4 0.8 0.9 1 1 reg:squarederror\n",
      "logloss 0.21066433333333334 for 499 rounds\n",
      "0.005 0.1 4 0.8 0.9 5 0.5 reg:logistic\n",
      "logloss 0.21227266666666667 for 499 rounds\n",
      "0.005 0.1 4 0.8 0.9 5 0.5 reg:squarederror\n",
      "logloss 0.210655 for 499 rounds\n",
      "0.005 0.1 4 0.8 0.9 5 1 reg:logistic\n",
      "logloss 0.21231866666666666 for 499 rounds\n",
      "0.005 0.1 4 0.8 0.9 5 1 reg:squarederror\n",
      "logloss 0.210684 for 499 rounds\n",
      "0.005 0.1 6 0.7 0.8 1 0.5 reg:logistic\n",
      "logloss 0.21042066666666667 for 499 rounds\n",
      "0.005 0.1 6 0.7 0.8 1 0.5 reg:squarederror\n",
      "logloss 0.20952300000000001 for 499 rounds\n",
      "0.005 0.1 6 0.7 0.8 1 1 reg:logistic\n",
      "logloss 0.21055100000000002 for 499 rounds\n",
      "0.005 0.1 6 0.7 0.8 1 1 reg:squarederror\n",
      "logloss 0.20962766666666666 for 499 rounds\n",
      "0.005 0.1 6 0.7 0.8 5 0.5 reg:logistic\n",
      "logloss 0.211037 for 499 rounds\n",
      "0.005 0.1 6 0.7 0.8 5 0.5 reg:squarederror\n",
      "logloss 0.20959666666666668 for 499 rounds\n",
      "0.005 0.1 6 0.7 0.8 5 1 reg:logistic\n",
      "logloss 0.21113666666666667 for 499 rounds\n",
      "0.005 0.1 6 0.7 0.8 5 1 reg:squarederror\n",
      "logloss 0.20971333333333333 for 499 rounds\n",
      "0.005 0.1 6 0.7 0.9 1 0.5 reg:logistic\n",
      "logloss 0.21038566666666667 for 499 rounds\n",
      "0.005 0.1 6 0.7 0.9 1 0.5 reg:squarederror\n",
      "logloss 0.20950099999999997 for 499 rounds\n",
      "0.005 0.1 6 0.7 0.9 1 1 reg:logistic\n",
      "logloss 0.21051399999999998 for 499 rounds\n",
      "0.005 0.1 6 0.7 0.9 1 1 reg:squarederror\n",
      "logloss 0.20959066666666668 for 499 rounds\n",
      "0.005 0.1 6 0.7 0.9 5 0.5 reg:logistic\n",
      "logloss 0.21098966666666666 for 499 rounds\n",
      "0.005 0.1 6 0.7 0.9 5 0.5 reg:squarederror\n",
      "logloss 0.20955900000000002 for 499 rounds\n",
      "0.005 0.1 6 0.7 0.9 5 1 reg:logistic\n",
      "logloss 0.21109966666666666 for 499 rounds\n",
      "0.005 0.1 6 0.7 0.9 5 1 reg:squarederror\n",
      "logloss 0.20967000000000002 for 499 rounds\n",
      "0.005 0.1 6 0.8 0.8 1 0.5 reg:logistic\n",
      "logloss 0.21042466666666668 for 499 rounds\n",
      "0.005 0.1 6 0.8 0.8 1 0.5 reg:squarederror\n",
      "logloss 0.20950466666666667 for 499 rounds\n",
      "0.005 0.1 6 0.8 0.8 1 1 reg:logistic\n",
      "logloss 0.21054399999999998 for 499 rounds\n",
      "0.005 0.1 6 0.8 0.8 1 1 reg:squarederror\n",
      "logloss 0.20959766666666665 for 499 rounds\n",
      "0.005 0.1 6 0.8 0.8 5 0.5 reg:logistic\n",
      "logloss 0.21098799999999998 for 499 rounds\n",
      "0.005 0.1 6 0.8 0.8 5 0.5 reg:squarederror\n",
      "logloss 0.20957666666666666 for 499 rounds\n",
      "0.005 0.1 6 0.8 0.8 5 1 reg:logistic\n",
      "logloss 0.21108866666666667 for 499 rounds\n",
      "0.005 0.1 6 0.8 0.8 5 1 reg:squarederror\n",
      "logloss 0.20967766666666665 for 499 rounds\n",
      "0.005 0.1 6 0.8 0.9 1 0.5 reg:logistic\n",
      "logloss 0.21039333333333335 for 499 rounds\n",
      "0.005 0.1 6 0.8 0.9 1 0.5 reg:squarederror\n",
      "logloss 0.20948433333333336 for 499 rounds\n",
      "0.005 0.1 6 0.8 0.9 1 1 reg:logistic\n",
      "logloss 0.21051966666666666 for 499 rounds\n",
      "0.005 0.1 6 0.8 0.9 1 1 reg:squarederror\n",
      "logloss 0.20957733333333337 for 499 rounds\n",
      "0.005 0.1 6 0.8 0.9 5 0.5 reg:logistic\n",
      "logloss 0.21094633333333335 for 499 rounds\n",
      "0.005 0.1 6 0.8 0.9 5 0.5 reg:squarederror\n",
      "logloss 0.20956066666666665 for 499 rounds\n",
      "0.005 0.1 6 0.8 0.9 5 1 reg:logistic\n",
      "logloss 0.21104933333333334 for 499 rounds\n",
      "0.005 0.1 6 0.8 0.9 5 1 reg:squarederror\n",
      "logloss 0.209642 for 499 rounds\n",
      "0.005 0.5 4 0.7 0.8 1 0.5 reg:logistic\n",
      "logloss 0.211985 for 499 rounds\n",
      "0.005 0.5 4 0.7 0.8 1 0.5 reg:squarederror\n",
      "logloss 0.210659 for 499 rounds\n",
      "0.005 0.5 4 0.7 0.8 1 1 reg:logistic\n",
      "logloss 0.21202766666666664 for 499 rounds\n",
      "0.005 0.5 4 0.7 0.8 1 1 reg:squarederror\n",
      "logloss 0.21070233333333332 for 499 rounds\n",
      "0.005 0.5 4 0.7 0.8 5 0.5 reg:logistic\n",
      "logloss 0.2122426666666667 for 499 rounds\n",
      "0.005 0.5 4 0.7 0.8 5 0.5 reg:squarederror\n",
      "logloss 0.21069233333333334 for 499 rounds\n",
      "0.005 0.5 4 0.7 0.8 5 1 reg:logistic\n",
      "logloss 0.21229566666666666 for 499 rounds\n",
      "0.005 0.5 4 0.7 0.8 5 1 reg:squarederror\n",
      "logloss 0.21073666666666666 for 499 rounds\n",
      "0.005 0.5 4 0.7 0.9 1 0.5 reg:logistic\n",
      "logloss 0.211996 for 499 rounds\n",
      "0.005 0.5 4 0.7 0.9 1 0.5 reg:squarederror\n",
      "logloss 0.210645 for 499 rounds\n",
      "0.005 0.5 4 0.7 0.9 1 1 reg:logistic\n",
      "logloss 0.212036 for 499 rounds\n",
      "0.005 0.5 4 0.7 0.9 1 1 reg:squarederror\n",
      "logloss 0.21068299999999998 for 499 rounds\n",
      "0.005 0.5 4 0.7 0.9 5 0.5 reg:logistic\n",
      "logloss 0.21225166666666664 for 499 rounds\n",
      "0.005 0.5 4 0.7 0.9 5 0.5 reg:squarederror\n",
      "logloss 0.2106726666666667 for 499 rounds\n",
      "0.005 0.5 4 0.7 0.9 5 1 reg:logistic\n",
      "logloss 0.21229900000000002 for 499 rounds\n",
      "0.005 0.5 4 0.7 0.9 5 1 reg:squarederror\n",
      "logloss 0.21071166666666663 for 499 rounds\n",
      "0.005 0.5 4 0.8 0.8 1 0.5 reg:logistic\n",
      "logloss 0.21202533333333332 for 499 rounds\n",
      "0.005 0.5 4 0.8 0.8 1 0.5 reg:squarederror\n",
      "logloss 0.21067099999999997 for 499 rounds\n",
      "0.005 0.5 4 0.8 0.8 1 1 reg:logistic\n",
      "logloss 0.21206233333333335 for 499 rounds\n",
      "0.005 0.5 4 0.8 0.8 1 1 reg:squarederror\n",
      "logloss 0.21070233333333332 for 499 rounds\n",
      "0.005 0.5 4 0.8 0.8 5 0.5 reg:logistic\n",
      "logloss 0.21225366666666667 for 499 rounds\n",
      "0.005 0.5 4 0.8 0.8 5 0.5 reg:squarederror\n",
      "logloss 0.21069433333333332 for 499 rounds\n",
      "0.005 0.5 4 0.8 0.8 5 1 reg:logistic\n",
      "logloss 0.21229866666666666 for 499 rounds\n",
      "0.005 0.5 4 0.8 0.8 5 1 reg:squarederror\n",
      "logloss 0.21073433333333336 for 499 rounds\n",
      "0.005 0.5 4 0.8 0.9 1 0.5 reg:logistic\n",
      "logloss 0.21204133333333333 for 499 rounds\n",
      "0.005 0.5 4 0.8 0.9 1 0.5 reg:squarederror\n",
      "logloss 0.21065566666666666 for 499 rounds\n",
      "0.005 0.5 4 0.8 0.9 1 1 reg:logistic\n",
      "logloss 0.21207433333333334 for 499 rounds\n",
      "0.005 0.5 4 0.8 0.9 1 1 reg:squarederror\n",
      "logloss 0.21069633333333335 for 499 rounds\n",
      "0.005 0.5 4 0.8 0.9 5 0.5 reg:logistic\n",
      "logloss 0.2122746666666667 for 499 rounds\n",
      "0.005 0.5 4 0.8 0.9 5 0.5 reg:squarederror\n",
      "logloss 0.21068699999999999 for 499 rounds\n",
      "0.005 0.5 4 0.8 0.9 5 1 reg:logistic\n",
      "logloss 0.21231999999999998 for 499 rounds\n",
      "0.005 0.5 4 0.8 0.9 5 1 reg:squarederror\n",
      "logloss 0.21071933333333334 for 499 rounds\n",
      "0.005 0.5 6 0.7 0.8 1 0.5 reg:logistic\n",
      "logloss 0.21042366666666668 for 499 rounds\n",
      "0.005 0.5 6 0.7 0.8 1 0.5 reg:squarederror\n",
      "logloss 0.20967766666666668 for 499 rounds\n",
      "0.005 0.5 6 0.7 0.8 1 1 reg:logistic\n",
      "logloss 0.210547 for 499 rounds\n",
      "0.005 0.5 6 0.7 0.8 1 1 reg:squarederror\n",
      "logloss 0.20979266666666666 for 499 rounds\n",
      "0.005 0.5 6 0.7 0.8 5 0.5 reg:logistic\n",
      "logloss 0.21103766666666668 for 499 rounds\n",
      "0.005 0.5 6 0.7 0.8 5 0.5 reg:squarederror\n",
      "logloss 0.20975633333333332 for 499 rounds\n",
      "0.005 0.5 6 0.7 0.8 5 1 reg:logistic\n",
      "logloss 0.21113733333333332 for 499 rounds\n",
      "0.005 0.5 6 0.7 0.8 5 1 reg:squarederror\n",
      "logloss 0.20989899999999997 for 499 rounds\n",
      "0.005 0.5 6 0.7 0.9 1 0.5 reg:logistic\n",
      "logloss 0.21039033333333335 for 499 rounds\n",
      "0.005 0.5 6 0.7 0.9 1 0.5 reg:squarederror\n",
      "logloss 0.20963733333333331 for 499 rounds\n",
      "0.005 0.5 6 0.7 0.9 1 1 reg:logistic\n",
      "logloss 0.210522 for 499 rounds\n",
      "0.005 0.5 6 0.7 0.9 1 1 reg:squarederror\n",
      "logloss 0.20974066666666666 for 499 rounds\n",
      "0.005 0.5 6 0.7 0.9 5 0.5 reg:logistic\n",
      "logloss 0.21099333333333334 for 499 rounds\n",
      "0.005 0.5 6 0.7 0.9 5 0.5 reg:squarederror\n",
      "logloss 0.2097146666666667 for 499 rounds\n",
      "0.005 0.5 6 0.7 0.9 5 1 reg:logistic\n",
      "logloss 0.211105 for 499 rounds\n",
      "0.005 0.5 6 0.7 0.9 5 1 reg:squarederror\n",
      "logloss 0.20983866666666665 for 499 rounds\n",
      "0.005 0.5 6 0.8 0.8 1 0.5 reg:logistic\n",
      "logloss 0.21042933333333333 for 499 rounds\n",
      "0.005 0.5 6 0.8 0.8 1 0.5 reg:squarederror\n",
      "logloss 0.20964533333333332 for 499 rounds\n",
      "0.005 0.5 6 0.8 0.8 1 1 reg:logistic\n",
      "logloss 0.2105463333333333 for 499 rounds\n",
      "0.005 0.5 6 0.8 0.8 1 1 reg:squarederror\n",
      "logloss 0.20976566666666666 for 499 rounds\n",
      "0.005 0.5 6 0.8 0.8 5 0.5 reg:logistic\n",
      "logloss 0.2109926666666667 for 499 rounds\n",
      "0.005 0.5 6 0.8 0.8 5 0.5 reg:squarederror\n",
      "logloss 0.2097256666666667 for 499 rounds\n",
      "0.005 0.5 6 0.8 0.8 5 1 reg:logistic\n",
      "logloss 0.21109266666666668 for 499 rounds\n",
      "0.005 0.5 6 0.8 0.8 5 1 reg:squarederror\n",
      "logloss 0.20984766666666665 for 499 rounds\n",
      "0.005 0.5 6 0.8 0.9 1 0.5 reg:logistic\n",
      "logloss 0.21040233333333333 for 499 rounds\n",
      "0.005 0.5 6 0.8 0.9 1 0.5 reg:squarederror\n",
      "logloss 0.20960233333333333 for 499 rounds\n",
      "0.005 0.5 6 0.8 0.9 1 1 reg:logistic\n",
      "logloss 0.2105203333333333 for 499 rounds\n",
      "0.005 0.5 6 0.8 0.9 1 1 reg:squarederror\n",
      "logloss 0.2097176666666667 for 499 rounds\n",
      "0.005 0.5 6 0.8 0.9 5 0.5 reg:logistic\n",
      "logloss 0.21094866666666665 for 499 rounds\n",
      "0.005 0.5 6 0.8 0.9 5 0.5 reg:squarederror\n",
      "logloss 0.20969433333333332 for 499 rounds\n",
      "0.005 0.5 6 0.8 0.9 5 1 reg:logistic\n",
      "logloss 0.21105433333333332 for 499 rounds\n",
      "0.005 0.5 6 0.8 0.9 5 1 reg:squarederror\n",
      "logloss 0.2098 for 499 rounds\n",
      "0.01 0.1 4 0.7 0.8 1 0.5 reg:logistic\n",
      "logloss 0.194007 for 499 rounds\n",
      "0.01 0.1 4 0.7 0.8 1 0.5 reg:squarederror\n",
      "logloss 0.19332066666666667 for 499 rounds\n",
      "0.01 0.1 4 0.7 0.8 1 1 reg:logistic\n",
      "logloss 0.19403366666666666 for 499 rounds\n",
      "0.01 0.1 4 0.7 0.8 1 1 reg:squarederror\n",
      "logloss 0.193311 for 499 rounds\n",
      "0.01 0.1 4 0.7 0.8 5 0.5 reg:logistic\n",
      "logloss 0.1940706666666667 for 499 rounds\n",
      "0.01 0.1 4 0.7 0.8 5 0.5 reg:squarederror\n",
      "logloss 0.1933446666666667 for 499 rounds\n",
      "0.01 0.1 4 0.7 0.8 5 1 reg:logistic\n",
      "logloss 0.194084 for 499 rounds\n",
      "0.01 0.1 4 0.7 0.8 5 1 reg:squarederror\n",
      "logloss 0.193338 for 499 rounds\n",
      "0.01 0.1 4 0.7 0.9 1 0.5 reg:logistic\n",
      "logloss 0.19400366666666666 for 499 rounds\n",
      "0.01 0.1 4 0.7 0.9 1 0.5 reg:squarederror\n",
      "logloss 0.193327 for 499 rounds\n",
      "0.01 0.1 4 0.7 0.9 1 1 reg:logistic\n",
      "logloss 0.19400333333333333 for 499 rounds\n",
      "0.01 0.1 4 0.7 0.9 1 1 reg:squarederror\n",
      "logloss 0.19331 for 499 rounds\n",
      "0.01 0.1 4 0.7 0.9 5 0.5 reg:logistic\n",
      "logloss 0.19406933333333334 for 499 rounds\n",
      "0.01 0.1 4 0.7 0.9 5 0.5 reg:squarederror\n",
      "logloss 0.19334033333333334 for 499 rounds\n",
      "0.01 0.1 4 0.7 0.9 5 1 reg:logistic\n",
      "logloss 0.19408566666666668 for 499 rounds\n",
      "0.01 0.1 4 0.7 0.9 5 1 reg:squarederror\n",
      "logloss 0.19332899999999997 for 499 rounds\n",
      "0.01 0.1 4 0.8 0.8 1 0.5 reg:logistic\n",
      "logloss 0.19408566666666668 for 499 rounds\n",
      "0.01 0.1 4 0.8 0.8 1 0.5 reg:squarederror\n",
      "logloss 0.19336566666666669 for 499 rounds\n",
      "0.01 0.1 4 0.8 0.8 1 1 reg:logistic\n",
      "logloss 0.194087 for 499 rounds\n",
      "0.01 0.1 4 0.8 0.8 1 1 reg:squarederror\n",
      "logloss 0.19336533333333336 for 499 rounds\n",
      "0.01 0.1 4 0.8 0.8 5 0.5 reg:logistic\n",
      "logloss 0.19413233333333332 for 499 rounds\n",
      "0.01 0.1 4 0.8 0.8 5 0.5 reg:squarederror\n",
      "logloss 0.19337233333333334 for 499 rounds\n",
      "0.01 0.1 4 0.8 0.8 5 1 reg:logistic\n",
      "logloss 0.194153 for 499 rounds\n",
      "0.01 0.1 4 0.8 0.8 5 1 reg:squarederror\n",
      "logloss 0.19336899999999999 for 499 rounds\n",
      "0.01 0.1 4 0.8 0.9 1 0.5 reg:logistic\n",
      "logloss 0.19408333333333336 for 499 rounds\n",
      "0.01 0.1 4 0.8 0.9 1 0.5 reg:squarederror\n",
      "logloss 0.193414 for 499 rounds\n",
      "0.01 0.1 4 0.8 0.9 1 1 reg:logistic\n",
      "logloss 0.194093 for 499 rounds\n",
      "0.01 0.1 4 0.8 0.9 1 1 reg:squarederror\n",
      "logloss 0.193398 for 499 rounds\n",
      "0.01 0.1 4 0.8 0.9 5 0.5 reg:logistic\n",
      "logloss 0.19415233333333334 for 499 rounds\n",
      "0.01 0.1 4 0.8 0.9 5 0.5 reg:squarederror\n",
      "logloss 0.19341566666666665 for 499 rounds\n",
      "0.01 0.1 4 0.8 0.9 5 1 reg:logistic\n",
      "logloss 0.194164 for 499 rounds\n",
      "0.01 0.1 4 0.8 0.9 5 1 reg:squarederror\n",
      "logloss 0.19340133333333331 for 499 rounds\n",
      "0.01 0.1 6 0.7 0.8 1 0.5 reg:logistic\n",
      "logloss 0.19315700000000002 for 499 rounds\n",
      "0.01 0.1 6 0.7 0.8 1 0.5 reg:squarederror\n",
      "logloss 0.1929436666666667 for 499 rounds\n",
      "0.01 0.1 6 0.7 0.8 1 1 reg:logistic\n",
      "logloss 0.193182 for 499 rounds\n",
      "0.01 0.1 6 0.7 0.8 1 1 reg:squarederror\n",
      "logloss 0.19300033333333336 for 499 rounds\n",
      "0.01 0.1 6 0.7 0.8 5 0.5 reg:logistic\n",
      "logloss 0.19324299999999997 for 499 rounds\n",
      "0.01 0.1 6 0.7 0.8 5 0.5 reg:squarederror\n",
      "logloss 0.19298966666666664 for 499 rounds\n",
      "0.01 0.1 6 0.7 0.8 5 1 reg:logistic\n",
      "logloss 0.19328966666666667 for 499 rounds\n",
      "0.01 0.1 6 0.7 0.8 5 1 reg:squarederror\n",
      "logloss 0.19302733333333336 for 499 rounds\n",
      "0.01 0.1 6 0.7 0.9 1 0.5 reg:logistic\n",
      "logloss 0.19314 for 499 rounds\n",
      "0.01 0.1 6 0.7 0.9 1 0.5 reg:squarederror\n",
      "logloss 0.19300733333333334 for 499 rounds\n",
      "0.01 0.1 6 0.7 0.9 1 1 reg:logistic\n",
      "logloss 0.193148 for 499 rounds\n",
      "0.01 0.1 6 0.7 0.9 1 1 reg:squarederror\n",
      "logloss 0.193048 for 499 rounds\n",
      "0.01 0.1 6 0.7 0.9 5 0.5 reg:logistic\n",
      "logloss 0.19326033333333337 for 499 rounds\n",
      "0.01 0.1 6 0.7 0.9 5 0.5 reg:squarederror\n",
      "logloss 0.1930236666666667 for 499 rounds\n",
      "0.01 0.1 6 0.7 0.9 5 1 reg:logistic\n",
      "logloss 0.19327933333333336 for 499 rounds\n",
      "0.01 0.1 6 0.7 0.9 5 1 reg:squarederror\n",
      "logloss 0.193024 for 499 rounds\n",
      "0.01 0.1 6 0.8 0.8 1 0.5 reg:logistic\n",
      "logloss 0.19323166666666666 for 499 rounds\n",
      "0.01 0.1 6 0.8 0.8 1 0.5 reg:squarederror\n",
      "logloss 0.1929506666666667 for 499 rounds\n",
      "0.01 0.1 6 0.8 0.8 1 1 reg:logistic\n",
      "logloss 0.19322499999999998 for 499 rounds\n",
      "0.01 0.1 6 0.8 0.8 1 1 reg:squarederror\n",
      "logloss 0.19291133333333332 for 499 rounds\n",
      "0.01 0.1 6 0.8 0.8 5 0.5 reg:logistic\n",
      "logloss 0.19331599999999996 for 499 rounds\n",
      "0.01 0.1 6 0.8 0.8 5 0.5 reg:squarederror\n",
      "logloss 0.19292066666666666 for 499 rounds\n",
      "0.01 0.1 6 0.8 0.8 5 1 reg:logistic\n",
      "logloss 0.19331500000000001 for 499 rounds\n",
      "0.01 0.1 6 0.8 0.8 5 1 reg:squarederror\n",
      "logloss 0.192927 for 499 rounds\n",
      "0.01 0.1 6 0.8 0.9 1 0.5 reg:logistic\n",
      "logloss 0.193248 for 499 rounds\n",
      "0.01 0.1 6 0.8 0.9 1 0.5 reg:squarederror\n",
      "logloss 0.19299533333333332 for 499 rounds\n",
      "0.01 0.1 6 0.8 0.9 1 1 reg:logistic\n",
      "logloss 0.19327166666666665 for 499 rounds\n",
      "0.01 0.1 6 0.8 0.9 1 1 reg:squarederror\n",
      "logloss 0.193012 for 499 rounds\n",
      "0.01 0.1 6 0.8 0.9 5 0.5 reg:logistic\n",
      "logloss 0.19332533333333332 for 499 rounds\n",
      "0.01 0.1 6 0.8 0.9 5 0.5 reg:squarederror\n",
      "logloss 0.19300033333333333 for 499 rounds\n",
      "0.01 0.1 6 0.8 0.9 5 1 reg:logistic\n",
      "logloss 0.19334733333333332 for 499 rounds\n",
      "0.01 0.1 6 0.8 0.9 5 1 reg:squarederror\n",
      "logloss 0.1929896666666667 for 499 rounds\n",
      "0.01 0.5 4 0.7 0.8 1 0.5 reg:logistic\n",
      "logloss 0.19400933333333334 for 499 rounds\n",
      "0.01 0.5 4 0.7 0.8 1 0.5 reg:squarederror\n",
      "logloss 0.19339966666666666 for 499 rounds\n",
      "0.01 0.5 4 0.7 0.8 1 1 reg:logistic\n",
      "logloss 0.194029 for 499 rounds\n",
      "0.01 0.5 4 0.7 0.8 1 1 reg:squarederror\n",
      "logloss 0.19338066666666667 for 499 rounds\n",
      "0.01 0.5 4 0.7 0.8 5 0.5 reg:logistic\n",
      "logloss 0.194066 for 499 rounds\n",
      "0.01 0.5 4 0.7 0.8 5 0.5 reg:squarederror\n",
      "logloss 0.19341866666666666 for 499 rounds\n",
      "0.01 0.5 4 0.7 0.8 5 1 reg:logistic\n",
      "logloss 0.19408433333333333 for 499 rounds\n",
      "0.01 0.5 4 0.7 0.8 5 1 reg:squarederror\n",
      "logloss 0.19340100000000002 for 499 rounds\n",
      "0.01 0.5 4 0.7 0.9 1 0.5 reg:logistic\n",
      "logloss 0.19399699999999998 for 499 rounds\n",
      "0.01 0.5 4 0.7 0.9 1 0.5 reg:squarederror\n",
      "logloss 0.1933976666666667 for 499 rounds\n",
      "0.01 0.5 4 0.7 0.9 1 1 reg:logistic\n",
      "logloss 0.19399933333333333 for 499 rounds\n",
      "0.01 0.5 4 0.7 0.9 1 1 reg:squarederror\n",
      "logloss 0.193376 for 499 rounds\n",
      "0.01 0.5 4 0.7 0.9 5 0.5 reg:logistic\n",
      "logloss 0.194067 for 499 rounds\n",
      "0.01 0.5 4 0.7 0.9 5 0.5 reg:squarederror\n",
      "logloss 0.193398 for 499 rounds\n",
      "0.01 0.5 4 0.7 0.9 5 1 reg:logistic\n",
      "logloss 0.19408166666666668 for 499 rounds\n",
      "0.01 0.5 4 0.7 0.9 5 1 reg:squarederror\n",
      "logloss 0.19343066666666667 for 499 rounds\n",
      "0.01 0.5 4 0.8 0.8 1 0.5 reg:logistic\n",
      "logloss 0.19408066666666665 for 499 rounds\n",
      "0.01 0.5 4 0.8 0.8 1 0.5 reg:squarederror\n",
      "logloss 0.19342633333333334 for 499 rounds\n",
      "0.01 0.5 4 0.8 0.8 1 1 reg:logistic\n",
      "logloss 0.19408266666666665 for 499 rounds\n",
      "0.01 0.5 4 0.8 0.8 1 1 reg:squarederror\n",
      "logloss 0.19342866666666667 for 499 rounds\n",
      "0.01 0.5 4 0.8 0.8 5 0.5 reg:logistic\n",
      "logloss 0.19413133333333335 for 499 rounds\n",
      "0.01 0.5 4 0.8 0.8 5 0.5 reg:squarederror\n",
      "logloss 0.19342833333333334 for 499 rounds\n",
      "0.01 0.5 4 0.8 0.8 5 1 reg:logistic\n",
      "logloss 0.19415300000000002 for 499 rounds\n",
      "0.01 0.5 4 0.8 0.8 5 1 reg:squarederror\n",
      "logloss 0.19344799999999998 for 499 rounds\n",
      "0.01 0.5 4 0.8 0.9 1 0.5 reg:logistic\n",
      "logloss 0.1940833333333333 for 499 rounds\n",
      "0.01 0.5 4 0.8 0.9 1 0.5 reg:squarederror\n",
      "logloss 0.19346233333333332 for 499 rounds\n",
      "0.01 0.5 4 0.8 0.9 1 1 reg:logistic\n",
      "logloss 0.19409533333333331 for 499 rounds\n",
      "0.01 0.5 4 0.8 0.9 1 1 reg:squarederror\n",
      "logloss 0.19346466666666665 for 499 rounds\n",
      "0.01 0.5 4 0.8 0.9 5 0.5 reg:logistic\n",
      "logloss 0.194149 for 499 rounds\n",
      "0.01 0.5 4 0.8 0.9 5 0.5 reg:squarederror\n",
      "logloss 0.19347733333333336 for 499 rounds\n",
      "0.01 0.5 4 0.8 0.9 5 1 reg:logistic\n",
      "logloss 0.19416633333333336 for 499 rounds\n",
      "0.01 0.5 4 0.8 0.9 5 1 reg:squarederror\n",
      "logloss 0.193488 for 499 rounds\n",
      "0.01 0.5 6 0.7 0.8 1 0.5 reg:logistic\n",
      "logloss 0.19315833333333332 for 499 rounds\n",
      "0.01 0.5 6 0.7 0.8 1 0.5 reg:squarederror\n",
      "logloss 0.19305333333333333 for 499 rounds\n",
      "0.01 0.5 6 0.7 0.8 1 1 reg:logistic\n",
      "logloss 0.19318766666666667 for 499 rounds\n",
      "0.01 0.5 6 0.7 0.8 1 1 reg:squarederror\n",
      "logloss 0.19302899999999998 for 499 rounds\n",
      "0.01 0.5 6 0.7 0.8 5 0.5 reg:logistic\n",
      "logloss 0.19326 for 499 rounds\n",
      "0.01 0.5 6 0.7 0.8 5 0.5 reg:squarederror\n",
      "logloss 0.19304633333333332 for 499 rounds\n",
      "0.01 0.5 6 0.7 0.8 5 1 reg:logistic\n",
      "logloss 0.19328533333333334 for 499 rounds\n",
      "0.01 0.5 6 0.7 0.8 5 1 reg:squarederror\n",
      "logloss 0.19306700000000002 for 499 rounds\n",
      "0.01 0.5 6 0.7 0.9 1 0.5 reg:logistic\n",
      "logloss 0.193126 for 499 rounds\n",
      "0.01 0.5 6 0.7 0.9 1 0.5 reg:squarederror\n",
      "logloss 0.1931143333333333 for 499 rounds\n",
      "0.01 0.5 6 0.7 0.9 1 1 reg:logistic\n",
      "logloss 0.19314699999999999 for 499 rounds\n",
      "0.01 0.5 6 0.7 0.9 1 1 reg:squarederror\n",
      "logloss 0.19305499999999998 for 499 rounds\n",
      "0.01 0.5 6 0.7 0.9 5 0.5 reg:logistic\n",
      "logloss 0.19326866666666667 for 499 rounds\n",
      "0.01 0.5 6 0.7 0.9 5 0.5 reg:squarederror\n",
      "logloss 0.19307566666666665 for 499 rounds\n",
      "0.01 0.5 6 0.7 0.9 5 1 reg:logistic\n",
      "logloss 0.193288 for 499 rounds\n",
      "0.01 0.5 6 0.7 0.9 5 1 reg:squarederror\n",
      "logloss 0.19305333333333333 for 499 rounds\n",
      "0.01 0.5 6 0.8 0.8 1 0.5 reg:logistic\n",
      "logloss 0.19322266666666668 for 499 rounds\n",
      "0.01 0.5 6 0.8 0.8 1 0.5 reg:squarederror\n",
      "logloss 0.192979 for 499 rounds\n",
      "0.01 0.5 6 0.8 0.8 1 1 reg:logistic\n",
      "logloss 0.19322733333333333 for 499 rounds\n",
      "0.01 0.5 6 0.8 0.8 1 1 reg:squarederror\n",
      "logloss 0.19303766666666666 for 499 rounds\n",
      "0.01 0.5 6 0.8 0.8 5 0.5 reg:logistic\n",
      "logloss 0.19332266666666667 for 499 rounds\n",
      "0.01 0.5 6 0.8 0.8 5 0.5 reg:squarederror\n",
      "logloss 0.19303800000000002 for 499 rounds\n",
      "0.01 0.5 6 0.8 0.8 5 1 reg:logistic\n",
      "logloss 0.19332133333333335 for 499 rounds\n",
      "0.01 0.5 6 0.8 0.8 5 1 reg:squarederror\n",
      "logloss 0.1930373333333333 for 499 rounds\n",
      "0.01 0.5 6 0.8 0.9 1 0.5 reg:logistic\n",
      "logloss 0.193244 for 499 rounds\n",
      "0.01 0.5 6 0.8 0.9 1 0.5 reg:squarederror\n",
      "logloss 0.19306800000000002 for 499 rounds\n",
      "0.01 0.5 6 0.8 0.9 1 1 reg:logistic\n",
      "logloss 0.193269 for 499 rounds\n",
      "0.01 0.5 6 0.8 0.9 1 1 reg:squarederror\n",
      "logloss 0.19304633333333332 for 499 rounds\n",
      "0.01 0.5 6 0.8 0.9 5 0.5 reg:logistic\n",
      "logloss 0.19332800000000003 for 499 rounds\n",
      "0.01 0.5 6 0.8 0.9 5 0.5 reg:squarederror\n",
      "logloss 0.19306333333333336 for 499 rounds\n",
      "0.01 0.5 6 0.8 0.9 5 1 reg:logistic\n",
      "logloss 0.19334766666666667 for 499 rounds\n",
      "0.01 0.5 6 0.8 0.9 5 1 reg:squarederror\n",
      "logloss 0.1930896666666667 for 499 rounds\n",
      "Best params:  (0.01, 0.1, 6, 0.8, 0.8, 1, 1, 'reg:squarederror') 0.19291133333333332\n"
     ]
    }
   ],
   "source": [
    "# read datasets\n",
    "train = data_train1.copy()\n",
    "test=data_test1.copy()\n",
    "order_id=test['order_id']\n",
    "y_train = train[\"is_none\"]\n",
    "\n",
    "dtrain = xgb.DMatrix(train.drop(['user_id','order_id','is_none'], axis=1), y_train)\n",
    "dtest = xgb.DMatrix(test.drop(['user_id','order_id'], axis=1))\n",
    "min_logloss = float(\"Inf\")\n",
    "\n",
    "grid_param = [(eta,gamma,max_depth,subsample,colsample_bytree,lambdaa,alpha,objective) \n",
    "              for eta in (0.005,0.01) for gamma in (0.1,0.5) for max_depth in (4,6) for subsample in (0.7,0.8) for colsample_bytree in (0.8,0.9) for lambdaa in (1,5) for alpha in (0.5,1) for objective in ('reg:logistic','reg:squarederror')]\n",
    "\n",
    "\n",
    "for eta,gamma,max_depth,subsample,colsample_bytree,lambdaa,alpha,objective in grid_param:\n",
    "    \n",
    "    xgb_params = {'eta':eta, \n",
    "              'gamma':gamma,\n",
    "              'max_depth':max_depth,\n",
    "              'subsample':subsample,\n",
    "              'colsample_bytree':colsample_bytree, \n",
    "              'lambda':lambdaa, \n",
    "              'alpha':alpha, \n",
    "              'objective':objective}\n",
    "    cv_results = xgb.cv(xgb_params, \n",
    "                   dtrain, \n",
    "                   nfold=3,\n",
    "                   num_boost_round=500, # increase to have better results (~700)\n",
    "                   early_stopping_rounds=50,\n",
    "                    metrics={'logloss'},\n",
    "                   show_stdv=False\n",
    "                  )\n",
    "     # Update best MAE\n",
    "    mean_logloss = cv_results['test-logloss-mean'].min()\n",
    "    boost_rounds = cv_results['test-logloss-mean'].argmin()\n",
    "    print(eta,gamma,max_depth,subsample,colsample_bytree,lambdaa,alpha,objective)\n",
    "    print(\"logloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = (eta,gamma,max_depth,subsample,colsample_bytree,lambdaa,alpha,objective)\n",
    "print(\"Best params: \", best_params, min_logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape train: (131209, 15)\n",
      " Shape test: (75000, 14)\n",
      "[0]\ttrain-logloss:0.68528\n",
      "[50]\ttrain-logloss:0.43004\n",
      "[100]\ttrain-logloss:0.31252\n",
      "[150]\ttrain-logloss:0.25284\n",
      "[200]\ttrain-logloss:0.22121\n",
      "[250]\ttrain-logloss:0.20408\n",
      "[300]\ttrain-logloss:0.19466\n",
      "[350]\ttrain-logloss:0.18928\n",
      "[400]\ttrain-logloss:0.18608\n",
      "[450]\ttrain-logloss:0.18403\n",
      "[500]\ttrain-logloss:0.18263\n",
      "[550]\ttrain-logloss:0.18160\n",
      "[600]\ttrain-logloss:0.18069\n",
      "[650]\ttrain-logloss:0.18014\n",
      "[700]\ttrain-logloss:0.17967\n",
      "[750]\ttrain-logloss:0.17900\n",
      "[800]\ttrain-logloss:0.17857\n",
      "[850]\ttrain-logloss:0.17796\n",
      "[900]\ttrain-logloss:0.17729\n",
      "[950]\ttrain-logloss:0.17666\n",
      "[1000]\ttrain-logloss:0.17605\n",
      "[1050]\ttrain-logloss:0.17546\n",
      "[1100]\ttrain-logloss:0.17461\n",
      "[1150]\ttrain-logloss:0.17401\n",
      "[1200]\ttrain-logloss:0.17347\n",
      "[1250]\ttrain-logloss:0.17288\n",
      "[1300]\ttrain-logloss:0.17231\n",
      "[1350]\ttrain-logloss:0.17196\n",
      "[1400]\ttrain-logloss:0.17124\n",
      "[1450]\ttrain-logloss:0.17074\n",
      "[1499]\ttrain-logloss:0.17021\n"
     ]
    }
   ],
   "source": [
    "# we are gonne predict None for the orders and store the probablity values into a file \n",
    "import xgboost as xgb\n",
    "# read datasets\n",
    "train = data_train1.copy()\n",
    "test=data_test1.copy()\n",
    "order_id=test['order_id']\n",
    "y_train = train[\"is_none\"]\n",
    "       \n",
    "print('Shape train: {}\\n Shape test: {}'.format(train.shape,test.shape))\n",
    "\n",
    "dtrain = xgb.DMatrix(train.drop(['user_id','order_id','is_none'], axis=1), y_train)\n",
    "dtest = xgb.DMatrix(test.drop(['user_id','order_id'], axis=1))\n",
    "\n",
    "xgb_params = {'eta':0.01, \n",
    "              'gamma':0.1,\n",
    "              'max_depth':6,\n",
    "              'subsample':0.8,\n",
    "              'colsample_bytree':0.8,\n",
    "              'lambda':1,\n",
    "              'alpha':1, \n",
    "              'objective':'reg:squarederror',\n",
    "             'eval_metric':'logloss'}\n",
    "\n",
    "watchlist= [(dtrain, \"train\")]\n",
    "num_boost_rounds=1500\n",
    "model = xgb.train(dict(xgb_params), dtrain, num_boost_round=num_boost_rounds, evals=watchlist, verbose_eval = 50,)\n",
    "preds=model.predict(dtest)\n",
    "\n",
    "out = pd.DataFrame({'order_id': order_id, 'none_pred': preds})\n",
    "out.to_csv('my_final/None_Probablity.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting model for predicting Products\n",
    "<p> like we did in previous files</p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all the features\n",
    "data_train1 = pd.read_csv('my_final/data.csv')\n",
    "labels_train = data_train1['reordered']\n",
    "\n",
    "# filling NaN values as -1\n",
    "data_train1 = data_train1.fillna(-1)\n",
    "\n",
    "data_test1 = pd.read_csv('my_final/test_data.csv')\n",
    "\n",
    "# filling NaN values as -1\n",
    "data_test1 = data_test1.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train1.drop(['Unnamed: 0', 'order_id','eval_set_x','eval_set_y','reordered','product_name'],axis=1)\n",
    "data_test = data_test1.drop(['Unnamed: 0', 'order_id','eval_set_x','eval_set_y','reordered','product_name'],axis=1)\n",
    "\n",
    "X_train = data_train.copy()\n",
    "y_train = labels_train.copy()\n",
    "X_Test = data_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:46:03] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.0.0\\src\\gbm\\gbtree.cc:138: Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "[0]\ttrain-logloss:0.51082\n",
      "[10]\ttrain-logloss:0.25091\n",
      "[20]\ttrain-logloss:0.24593\n",
      "[29]\ttrain-logloss:0.24491\n"
     ]
    }
   ],
   "source": [
    "# #read Data files\n",
    "# data_train1 = pd.read_csv('my_final/train_none_Data.csv')\n",
    "# data_test1 = pd.read_csv('my_final/test_none_Data.csv')\n",
    "\n",
    "xgb_params = {\n",
    "    \"objective\"         : \"reg:logistic\"\n",
    "    ,\"eval_metric\"      : \"logloss\"\n",
    "    ,\"max_depth\"        : 6\n",
    "    ,\"subsample\"        :0.8\n",
    "    ,\"colsample_bytree\" :0.8\n",
    "    ,\"alpha\"            :5\n",
    "    ,\"lambda\"           :1\n",
    "}\n",
    "num_round = 30\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, y_train)\n",
    "watchlist= [(d_train, \"train\")]\n",
    "bst = xgb.train(params= xgb_params, dtrain=d_train, num_boost_round=num_round, evals=watchlist,verbose_eval = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 optimization code to get the optimal f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the paper Optimizing F-measure: A Tale of Two Approaches - https://arxiv.org/abs/1206.4625\n",
    "# https://www.kaggle.com/mmueller/f1-score-expectation-maximization-in-o-n\n",
    "\"\"\"\n",
    "@author: Faron\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "'''\n",
    "This kernel implements the O(n²) F1-Score expectation maximization algorithm presented in\n",
    "\"Ye, N., Chai, K., Lee, W., and Chieu, H.  Optimizing F-measures: A Tale of Two Approaches. In ICML, 2012.\"\n",
    "\n",
    "It solves argmax_(0 <= k <= n,[[None]]) E[F1(P,k,[[None]])]\n",
    "with [[None]] being the indicator for predicting label \"None\"\n",
    "given posteriors P = [p_1, p_2, ... , p_n], where p_1 > p_2 > ... > p_n\n",
    "under label independence assumption by means of dynamic programming in O(n²).\n",
    "'''\n",
    "\n",
    "\n",
    "class F1Optimizer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def get_expectations(P, pNone=None):\n",
    "        expectations = []\n",
    "        P = np.sort(P)[::-1]\n",
    "\n",
    "        n = np.array(P).shape[0]\n",
    "        DP_C = np.zeros((n + 2, n + 1))\n",
    "        if pNone is None:\n",
    "            pNone = (1.0 - P).prod()\n",
    "\n",
    "        DP_C[0][0] = 1.0\n",
    "        for j in range(1, n):\n",
    "            DP_C[0][j] = (1.0 - P[j - 1]) * DP_C[0, j - 1]\n",
    "\n",
    "        for i in range(1, n + 1):\n",
    "            DP_C[i, i] = DP_C[i - 1, i - 1] * P[i - 1]\n",
    "            for j in range(i + 1, n + 1):\n",
    "                DP_C[i, j] = P[j - 1] * DP_C[i - 1, j - 1] + (1.0 - P[j - 1]) * DP_C[i, j - 1]\n",
    "\n",
    "        DP_S = np.zeros((2 * n + 1,))\n",
    "        DP_SNone = np.zeros((2 * n + 1,))\n",
    "        for i in range(1, 2 * n + 1):\n",
    "            DP_S[i] = 1. / (1. * i)\n",
    "            DP_SNone[i] = 1. / (1. * i + 1)\n",
    "        for k in range(n + 1)[::-1]:\n",
    "            f1 = 0\n",
    "            f1None = 0\n",
    "            for k1 in range(n + 1):\n",
    "                f1 += 2 * k1 * DP_C[k1][k] * DP_S[k + k1]\n",
    "                f1None += 2 * k1 * DP_C[k1][k] * DP_SNone[k + k1]\n",
    "            for i in range(1, 2 * k - 1):\n",
    "                DP_S[i] = (1 - P[k - 1]) * DP_S[i] + P[k - 1] * DP_S[i + 1]\n",
    "                DP_SNone[i] = (1 - P[k - 1]) * DP_SNone[i] + P[k - 1] * DP_SNone[i + 1]\n",
    "            expectations.append([f1None + 2 * pNone / (2 + k), f1])\n",
    "\n",
    "        return np.array(expectations[::-1]).T\n",
    "\n",
    "    @staticmethod\n",
    "    def maximize_expectation(P, pNone=None):\n",
    "        expectations = F1Optimizer.get_expectations(P, pNone)\n",
    "\n",
    "        ix_max = np.unravel_index(expectations.argmax(), expectations.shape)\n",
    "        max_f1 = expectations[ix_max]\n",
    "\n",
    "        predNone = True if ix_max[0] == 0 else False\n",
    "        best_k = ix_max[1]\n",
    "\n",
    "        return best_k, predNone, max_f1\n",
    "\n",
    "    @staticmethod\n",
    "    def _F1(tp, fp, fn):\n",
    "        return 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "    @staticmethod\n",
    "    def _Fbeta(tp, fp, fn, beta=1.0):\n",
    "        beta_squared = beta ** 2\n",
    "        return (1.0 + beta_squared) * tp / ((1.0 + beta_squared) * tp + fp + beta_squared * fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the probablities for two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "75000it [55:26, 22.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# merging the probablities for two models and passing them to the f1 optimizer and saving the results in the file for submission\n",
    "\n",
    "none_prob = pd.read_csv(\"my_final/None_Probablity.csv\")\n",
    "df_to_test = data_test1.copy()\n",
    "df_to_test1 =df_to_test[['order_id','product_id']]\n",
    "data_test = data_test1.drop(['Unnamed: 0', 'order_id','eval_set_x','eval_set_y','reordered','product_name'],axis=1)\n",
    "\n",
    "preds = bst.predict(xgb.DMatrix(data_test))\n",
    "\n",
    "df_to_test1['pred'] = preds\n",
    "df_to_test = df_to_test1.copy()\n",
    "\n",
    "df_to_test = df_to_test[['order_id','product_id','pred']].groupby('order_id').agg({'product_id': list, 'pred': list})\n",
    "\n",
    "def sort_values(row):  \n",
    "    ok = np.argsort(np.array(row['pred']))\n",
    "    df_to_test.at[row.name,'product_id']=[row['product_id'][i] for i in ok][::-1]\n",
    "    df_to_test.at[row.name,'pred'] = [row['pred'][i] for i in ok][::-1]\n",
    "\n",
    "df_to_test.apply(sort_values,axis=1)\n",
    "\n",
    "final_preds=[]\n",
    "final_order_ids=[]\n",
    "for index, row in tqdm(df_to_test.iterrows()):\n",
    "    final_order_ids.append(index)\n",
    "    opt=F1Optimizer.maximize_expectation(row.pred,none_prob[none_prob['order_id']==index]['none_pred'].values[0])\n",
    "    best_k=opt[0]\n",
    "    pred=df_to_test[df_to_test.index==index]['product_id'].values[0][:best_k]\n",
    "    if(opt[1]):\n",
    "        pred.append('None')\n",
    "    final_preds.append(' '.join([str(v) for v in pred]))\n",
    "submit=pd.DataFrame({'order_id': final_order_ids, 'products': final_preds})\n",
    "submit=submit.sort_values('order_id')\n",
    "submit.to_csv('my_final/sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Submission result\n",
    "<img src='my_final/GB_None_faron.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulic leader Board rank 526\n",
    "<img src='my_final/public.PNG'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
