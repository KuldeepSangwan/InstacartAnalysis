{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all the features\n",
    "data_train1 = pd.read_csv('my_final/data.csv')\n",
    "labels_train = data_train1['reordered']\n",
    "\n",
    "# filling NaN values as -1\n",
    "data_train1 = data_train1.fillna(-1)\n",
    "\n",
    "data_test1 = pd.read_csv('my_final/test_data.csv')\n",
    "\n",
    "# filling NaN values as -1\n",
    "data_test1 = data_test1.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train1.drop(['Unnamed: 0', 'order_id','eval_set_x','eval_set_y','reordered','product_name'],axis=1)\n",
    "data_test = data_test1.drop(['Unnamed: 0', 'order_id','eval_set_x','eval_set_y','reordered','product_name'],axis=1)\n",
    "\n",
    "X_train = data_train.copy()\n",
    "y_train = labels_train.copy()\n",
    "X_cv = data_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_Model = LogisticRegression(n_jobs=-1).fit(X_train,y_train)\n",
    "pred_Xcv = LR_Model.predict(X_cv)\n",
    "data_test1['pred'] = pred_Xcv\n",
    "test = data_test1.copy()\n",
    "test = test[test['pred']==True]\n",
    "test = test[['order_id','product_id','pred']].groupby('order_id').agg({'product_id': list, 'pred': list})\n",
    "test['products'] = test['product_id'].apply(lambda x: ' '.join(str(a) for a in x))\n",
    "here = test[['products']].merge(data_test1[['order_id']].drop_duplicates(subset='order_id', keep=\"last\"),on='order_id', how='outer')\n",
    "here = here.fillna('None')\n",
    "here.to_csv('my_final/LRSub.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_Model = LogisticRegression(class_weight = \"balanced\", n_jobs=-1).fit(X_train,y_train)\n",
    "pred_Xcv = LR_Model.predict(X_cv)\n",
    "data_test1['pred'] = pred_Xcv\n",
    "test = data_test1.copy()\n",
    "test = test[test['pred']==True]\n",
    "test = test[['order_id','product_id','pred']].groupby('order_id').agg({'product_id': list, 'pred': list})\n",
    "test['products'] = test['product_id'].apply(lambda x: ' '.join(str(a) for a in x))\n",
    "here = test[['products']].merge(data_test1[['order_id']].drop_duplicates(subset='order_id', keep=\"last\"),on='order_id', how='outer')\n",
    "here = here.fillna('None')\n",
    "here.to_csv('my_final/LRWeighSub.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB().fit(X_train, y_train)\n",
    "pred_Xcv = gnb.predict(X_cv)\n",
    "data_test1['pred'] = pred_Xcv\n",
    "test = data_test1.copy()\n",
    "test = test[test['pred']==True]\n",
    "test = test[['order_id','product_id','pred']].groupby('order_id').agg({'product_id': list, 'pred': list})\n",
    "test['products'] = test['product_id'].apply(lambda x: ' '.join(str(a) for a in x))\n",
    "here = test[['products']].merge(data_test1[['order_id']].drop_duplicates(subset='order_id', keep=\"last\"),on='order_id', how='outer')\n",
    "here = here.fillna('None')\n",
    "here.to_csv('my_final/NBSub.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(class_weight='balanced').fit(X_train, y_train)\n",
    "pred_Xcv = clf.predict(X_cv)\n",
    "data_test1['pred'] = pred_Xcv\n",
    "test = data_test1.copy()\n",
    "test = test[test['pred']==True]\n",
    "test = test[['order_id','product_id','pred']].groupby('order_id').agg({'product_id': list, 'pred': list})\n",
    "test['products'] = test['product_id'].apply(lambda x: ' '.join(str(a) for a in x))\n",
    "here = test[['products']].merge(data_test1[['order_id']].drop_duplicates(subset='order_id', keep=\"last\"),on='order_id', how='outer')\n",
    "here = here.fillna('None')\n",
    "here.to_csv('my_final/DTSub.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_clf = RandomForestClassifier(class_weight='balanced', n_jobs=-1).fit(X_train, y_train)\n",
    "pred_Xcv = rand_clf.predict(X_cv)\n",
    "data_test1['pred'] = pred_Xcv\n",
    "test = data_test1.copy()\n",
    "test = test[test['pred']==True]\n",
    "test = test[['order_id','product_id','pred']].groupby('order_id').agg({'product_id': list, 'pred': list})\n",
    "test['products'] = test['product_id'].apply(lambda x: ' '.join(str(a) for a in x))\n",
    "here = test[['products']].merge(data_test1[['order_id']].drop_duplicates(subset='order_id', keep=\"last\"),on='order_id', how='outer')\n",
    "here = here.fillna('None')\n",
    "here.to_csv('my_final/RFSub.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    \"objective\"         : \"reg:logistic\"\n",
    "    ,\"eval_metric\"      : \"logloss\"\n",
    "}\n",
    "num_round = 20\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, y_train)\n",
    "watchlist= [(d_train, \"train\")]\n",
    "bst = xgb.train(params= xgb_params, dtrain=d_train, num_boost_round=num_round, evals=watchlist,verbose_eval = 10)\n",
    "pred_Xcv = bst.predict(xgb.DMatrix(X_cv))\n",
    "pred_Xcv = [True if i >=0.16 else False for i in pred_Xcv]\n",
    "data_test1['pred'] = pred_Xcv\n",
    "test = data_test1.copy()\n",
    "test = test[test['pred']==True]\n",
    "test = test[['order_id','product_id','pred']].groupby('order_id').agg({'product_id': list, 'pred': list})\n",
    "test['products'] = test['product_id'].apply(lambda x: ' '.join(str(a) for a in x))\n",
    "here = test[['products']].merge(data_test1[['order_id']].drop_duplicates(subset='order_id', keep=\"last\"),on='order_id', how='outer')\n",
    "here = here.fillna('None')\n",
    "here.to_csv('my_final/GBSub.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>As the from the F1 Score we can get, that best model is Gradient descent with XGBoost. So first I will apply Gradient desent on Autoencoders and if we even can use an Autoencoder or not, then I will do the hyperparameter tuning on the model.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying autoencoder features with Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoencoder with Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "[16:44:38] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.0.0\\src\\gbm\\gbtree.cc:138: Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "[0]\ttrain-logloss:0.52407\n",
      "[10]\ttrain-logloss:0.29213\n",
      "[14]\ttrain-logloss:0.28840\n"
     ]
    }
   ],
   "source": [
    "# first we need to divide the data for the features that we will apply autencoding to, then merge back\n",
    "\n",
    "data_train1 = pd.read_csv('my_final/data.csv')\n",
    "labels_train1 = data_train1['reordered']\n",
    "# filling NaN values as -1\n",
    "data_train1 = data_train1.fillna(-1)\n",
    "\n",
    "data_test1 = pd.read_csv('my_final/test_data.csv')\n",
    "# filling NaN values as -1\n",
    "data_test1 = data_test1.fillna(-1)\n",
    "\n",
    "\n",
    "data_train = data_train1.drop(['Unnamed: 0', 'order_id','eval_set_x','eval_set_y','reordered','product_name','user_id','product_id','order_number','order_dow','order_hour_of_day','days_since_prior_order','aisle_id','department_id'],axis=1)\n",
    "data_test = data_test1.drop(['Unnamed: 0', 'order_id','eval_set_x','eval_set_y','reordered','product_name','user_id','product_id','order_number','order_dow','order_hour_of_day','days_since_prior_order','aisle_id','department_id'],axis=1)\n",
    "X_trainEncod = data_train.copy()\n",
    "y_trainEncod = labels_train.copy()\n",
    "X_cvEncod = data_test.copy()\n",
    "\n",
    "# the enoder we trained \n",
    "encoder = load_model('autoencoderwithNorm.h5')\n",
    "\n",
    "# tranforming our data\n",
    "X_trainEncod = encoder.predict(X_trainEncod)\n",
    "X_cvEncod = encoder.predict(X_cvEncod)\n",
    "\n",
    "X_trainMerge = data_train1[['user_id','product_id','order_number','order_dow','order_hour_of_day','days_since_prior_order','aisle_id','department_id']]\n",
    "X_CVMerge = data_test1[['user_id','product_id','order_number','order_dow','order_hour_of_day','days_since_prior_order','aisle_id','department_id']]\n",
    "\n",
    "X_trainEncod = pd.DataFrame(data=X_trainEncod, columns=list(range(34)))\n",
    "X_trainMerge= X_trainMerge.reset_index(drop=True)\n",
    "X_train = X_trainMerge.merge(X_trainEncod, left_index=True, right_index=True)\n",
    "\n",
    "X_cvEncod = pd.DataFrame(data=X_cvEncod, columns=list(range(34)))\n",
    "X_CVMerge= X_CVMerge.reset_index(drop=True)\n",
    "X_CV = X_CVMerge.merge(X_cvEncod, left_index=True, right_index=True)\n",
    "\n",
    "# Gradient Boosting to the Autoencoder\n",
    "xgb_params = {\n",
    "    \"objective\"         : \"reg:logistic\"\n",
    "    ,\"eval_metric\"      : \"logloss\"\n",
    "\n",
    "}\n",
    "num_round = 15\n",
    "d_train = xgb.DMatrix(X_train, y_trainEncod)\n",
    "watchlist= [(d_train, \"train\")]\n",
    "bst = xgb.train(params= xgb_params, dtrain=d_train,num_boost_round=num_round, evals=watchlist,verbose_eval = 10)\n",
    "pred_Xcv = bst.predict(xgb.DMatrix(X_CV))\n",
    "arr = [True if i >=0.16 else False for i in pred_Xcv]\n",
    "\n",
    "data_test1['pred'] = arr\n",
    "test = data_test1.copy()\n",
    "test = test[test['pred']==True]\n",
    "test = test[['order_id','product_id','pred']].groupby('order_id').agg({'product_id': list, 'pred': list})\n",
    "test['products'] = test['product_id'].apply(lambda x: ' '.join(str(a) for a in x))\n",
    "here = test[['products']].merge(data_test1[['order_id']].drop_duplicates(subset='order_id', keep=\"last\"),on='order_id', how='outer')\n",
    "here = here.fillna('None')\n",
    "here.to_csv('my_final/GBAutoEnodWithNormSub.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder without Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "[20:04:37] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.0.0\\src\\gbm\\gbtree.cc:138: Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "[0]\ttrain-logloss:0.52333\n",
      "[10]\ttrain-logloss:0.29256\n",
      "[14]\ttrain-logloss:0.28910\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder without Normalization \n",
    "\n",
    "# first we need to divide the data for the features that we will apply autencoding to, then merge back.\n",
    "\n",
    "data_train1 = pd.read_csv('my_final/data.csv')\n",
    "labels_train1 = data_train1['reordered']\n",
    "# filling NaN values as -1\n",
    "data_train1 = data_train1.fillna(-1)\n",
    "\n",
    "data_test1 = pd.read_csv('my_final/test_data.csv')\n",
    "# filling NaN values as -1\n",
    "data_test1 = data_test1.fillna(-1)\n",
    "\n",
    "\n",
    "data_train = data_train1.drop(['Unnamed: 0', 'order_id','eval_set_x','eval_set_y','reordered','product_name','user_id','product_id','order_number','order_dow','order_hour_of_day','days_since_prior_order','aisle_id','department_id'],axis=1)\n",
    "data_test = data_test1.drop(['Unnamed: 0', 'order_id','eval_set_x','eval_set_y','reordered','product_name','user_id','product_id','order_number','order_dow','order_hour_of_day','days_since_prior_order','aisle_id','department_id'],axis=1)\n",
    "X_trainEncod = data_train.copy()\n",
    "y_trainEncod = labels_train.copy()\n",
    "X_cvEncod = data_test.copy()\n",
    "\n",
    "# the enoder we trained \n",
    "encoder = load_model('autoencoder_withoutNorm.h5')\n",
    "\n",
    "# tranforming our data\n",
    "X_trainEncod = encoder.predict(X_trainEncod)\n",
    "X_cvEncod = encoder.predict(X_cvEncod)\n",
    "\n",
    "X_trainMerge = data_train1[['user_id','product_id','order_number','order_dow','order_hour_of_day','days_since_prior_order','aisle_id','department_id']]\n",
    "X_CVMerge = data_test1[['user_id','product_id','order_number','order_dow','order_hour_of_day','days_since_prior_order','aisle_id','department_id']]\n",
    "\n",
    "X_trainEncod = pd.DataFrame(data=X_trainEncod, columns=list(range(34)))\n",
    "X_trainMerge= X_trainMerge.reset_index(drop=True)\n",
    "X_train = X_trainMerge.merge(X_trainEncod, left_index=True, right_index=True)\n",
    "\n",
    "X_cvEncod = pd.DataFrame(data=X_cvEncod, columns=list(range(34)))\n",
    "X_CVMerge= X_CVMerge.reset_index(drop=True)\n",
    "X_CV = X_CVMerge.merge(X_cvEncod, left_index=True, right_index=True)\n",
    "\n",
    "# Gradient Boosting to the Autoencoder\n",
    "xgb_params = {\n",
    "    \"objective\"         : \"reg:logistic\"\n",
    "    ,\"eval_metric\"      : \"logloss\"\n",
    "\n",
    "}\n",
    "num_round = 15\n",
    "d_train = xgb.DMatrix(X_train, y_trainEncod)\n",
    "watchlist= [(d_train, \"train\")]\n",
    "bst = xgb.train(params= xgb_params, dtrain=d_train,num_boost_round=num_round, evals=watchlist,verbose_eval = 10)\n",
    "pred_Xcv = bst.predict(xgb.DMatrix(X_CV))\n",
    "arr = [True if i >=0.16 else False for i in pred_Xcv]\n",
    "\n",
    "data_test1['pred'] = arr\n",
    "test = data_test1.copy()\n",
    "test = test[test['pred']==True]\n",
    "test = test[['order_id','product_id','pred']].groupby('order_id').agg({'product_id': list, 'pred': list})\n",
    "test['products'] = test['product_id'].apply(lambda x: ' '.join(str(a) for a in x))\n",
    "here = test[['products']].merge(data_test1[['order_id']].drop_duplicates(subset='order_id', keep=\"last\"),on='order_id', how='outer')\n",
    "here = here.fillna('None')\n",
    "here.to_csv('my_final/GBAutoEnodWithoutNormSub.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The encoding of the features doesn't perform as good as the actual features,So it would better to use the actual features instead of encoded ones.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:38:03] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.0.0\\src\\gbm\\gbtree.cc:138: Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "[0]\ttrain-logloss:0.51082\n",
      "[10]\ttrain-logloss:0.25091\n",
      "[20]\ttrain-logloss:0.24593\n",
      "[30]\ttrain-logloss:0.24484\n",
      "[40]\ttrain-logloss:0.24409\n",
      "[50]\ttrain-logloss:0.24349\n",
      "[60]\ttrain-logloss:0.24297\n",
      "[70]\ttrain-logloss:0.24260\n",
      "[80]\ttrain-logloss:0.24224\n",
      "[90]\ttrain-logloss:0.24194\n",
      "[100]\ttrain-logloss:0.24166\n",
      "[110]\ttrain-logloss:0.24141\n",
      "[119]\ttrain-logloss:0.24119\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    \"objective\"         : \"reg:logistic\"\n",
    "    ,\"eval_metric\"      : \"logloss\"\n",
    "    ,\"max_depth\"        : 6\n",
    "    ,\"subsample\"        :0.8\n",
    "    ,\"colsample_bytree\" :0.8\n",
    "    ,\"alpha\"            :5\n",
    "    ,\"lambda\"           :1\n",
    "}\n",
    "num_round = 120\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, y_train)\n",
    "watchlist= [(d_train, \"train\")]\n",
    "bst = xgb.train(params= xgb_params, dtrain=d_train, num_boost_round=num_round, evals=watchlist,verbose_eval = 10)\n",
    "pred_Xcv = bst.predict(xgb.DMatrix(X_cv))\n",
    "pred_Xcv = [True if i >=0.16 else False for i in pred_Xcv]\n",
    "data_test1['pred'] = pred_Xcv\n",
    "test = data_test1.copy()\n",
    "test = test[test['pred']==True]\n",
    "test = test[['order_id','product_id','pred']].groupby('order_id').agg({'product_id': list, 'pred': list})\n",
    "test['products'] = test['product_id'].apply(lambda x: ' '.join(str(a) for a in x))\n",
    "here = test[['products']].merge(data_test1[['order_id']].drop_duplicates(subset='order_id', keep=\"last\"),on='order_id', how='outer')\n",
    "here = here.fillna('None')\n",
    "here.to_csv('my_final/GBBestSub.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kaggle Private and Public Scores for Test data with Different Models \n",
    "<img src='my_final/Capture.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 76)]              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 50)                3850      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 5,151\n",
      "Trainable params: 5,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "   2/2825 [..............................] - ETA: 45:16 - loss: 2794.7822 - f1_score: 0.0000e+00 - accuracy: 0.9018WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0060s vs `on_train_batch_begin` time: 0.0611s). Check your callbacks.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0060s vs `on_train_batch_end` time: 1.8093s). Check your callbacks.\n",
      "2825/2825 [==============================] - 11s 4ms/step - loss: 45.6903 - f1_score: 0.1690 - accuracy: 0.8358\n",
      "Epoch 2/10\n",
      "2825/2825 [==============================] - 9s 3ms/step - loss: 0.4112 - f1_score: 0.1782 - accuracy: 0.8957\n",
      "Epoch 3/10\n",
      "2825/2825 [==============================] - 9s 3ms/step - loss: 0.2941 - f1_score: 0.1782 - accuracy: 0.9010\n",
      "Epoch 4/10\n",
      "2825/2825 [==============================] - 9s 3ms/step - loss: 0.2784 - f1_score: 0.1782 - accuracy: 0.9021\n",
      "Epoch 5/10\n",
      "2825/2825 [==============================] - 9s 3ms/step - loss: 0.2713 - f1_score: 0.1782 - accuracy: 0.9022\n",
      "Epoch 6/10\n",
      "2825/2825 [==============================] - 9s 3ms/step - loss: 0.2751 - f1_score: 0.1782 - accuracy: 0.9020\n",
      "Epoch 7/10\n",
      "2825/2825 [==============================] - 9s 3ms/step - loss: 0.3205 - f1_score: 0.1782 - accuracy: 0.9021\n",
      "Epoch 8/10\n",
      "2825/2825 [==============================] - 9s 3ms/step - loss: 0.2971 - f1_score: 0.1782 - accuracy: 0.9022\n",
      "Epoch 9/10\n",
      "2825/2825 [==============================] - 9s 3ms/step - loss: 0.2809 - f1_score: 0.1782 - accuracy: 0.9022\n",
      "Epoch 10/10\n",
      "2825/2825 [==============================] - 10s 3ms/step - loss: 0.2797 - f1_score: 0.1782 - accuracy: 0.9022: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x241fbec6c18>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##imports\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "import datetime\n",
    "\n",
    "##create an NN and \n",
    "x_in = Input(shape=(76,))\n",
    "x_dense1 = Dense(50, activation=\"relu\")(x_in)\n",
    "x_dropout = Dropout(0.1)(x_dense1)\n",
    "x_dense2 = Dense(25, activation=\"relu\")(x_dropout)\n",
    "x_out = Dense(1, activation='sigmoid')(x_dense2)\n",
    "\n",
    "model = Model(inputs=x_in, outputs=x_out)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[F1Score(num_classes=1),'accuracy'])\n",
    "\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(X_train, np.where(y_train.values==True,1,0), epochs=10, batch_size=3000, verbose=1, callbacks=[tensorboard_callback])\n",
    "\n",
    "pred = model.predict(X_cv,verbose=1, batch_size=1000)\n",
    "pred_Xcv = [True if i >=0.16 else False for i in pred]\n",
    "data_test1['pred'] = pred_Xcv\n",
    "test = data_test1.copy()\n",
    "test = test[test['pred']==True]\n",
    "test = test[['order_id','product_id','pred']].groupby('order_id').agg({'product_id': list, 'pred': list})\n",
    "test['products'] = test['product_id'].apply(lambda x: ' '.join(str(a) for a in x))\n",
    "here = test[['products']].merge(data_test1[['order_id']].drop_duplicates(subset='order_id', keep=\"last\"),on='order_id', how='outer')\n",
    "here = here.fillna('None')\n",
    "here.to_csv('temp_fold/NNSub1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kaggle Private and Public Scores for NN\n",
    "<img src='my_final/NN_kaggle.PNG'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
